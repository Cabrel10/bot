# Guide du Mod√®le Neuronal

## üß† Architecture du R√©seau

### Configuration de Base

1. **Structure des Couches**
   ```yaml
   architecture:
     layers:
       - size: 128
         activation: "swish"
         dropout: 0.3
         batch_norm: true
       - size: 64
         activation: "swish"
         dropout: 0.2
         batch_norm: true
       - size: 32
         activation: "swish"
         dropout: 0.1
         batch_norm: true
   ```
   - Couches denses d√©croissantes
   - Activation Swish pour la non-lin√©arit√©
   - Dropout pour la r√©gularisation
   - Batch Normalization pour la stabilit√©

2. **Couches Sp√©ciales**
   ```yaml
   special_layers:
     type: "TCN"  # Temporal Convolutional Network
     params:
       kernel_size: 3
       dilations: [1, 2, 4, 8]
       nb_filters: 64
   ```
   - TCN pour le traitement temporel
   - Dilatations croissantes
   - Filtres convolutionnels

## ‚öôÔ∏è Entra√Ænement

### Param√®tres d'Entra√Ænement

1. **Optimiseur**
   ```python
   from trading.models import NeuralNetwork
   
   model = NeuralNetwork()
   # Configuration automatique depuis le YAML:
   # optimizer:
   #   type: "adam"
   #   learning_rate: 0.001
   #   beta_1: 0.9
   #   beta_2: 0.999
   ```

2. **Hyperparam√®tres**
   ```yaml
   training:
     batch_size: 256
     epochs: 100
     loss_function: "huber"
     metrics:
       - "mae"
       - "mse"
       - "sharpe_ratio"
   ```

### R√©gularisation

1. **Early Stopping**
   ```yaml
   regularization:
     early_stopping:
       patience: 10
       min_delta: 0.001
   ```
   - Arr√™t anticip√© si pas d'am√©lioration
   - Restauration des meilleurs poids

2. **Learning Rate Scheduler**
   ```yaml
   learning_rate_scheduler:
     type: "reduce_on_plateau"
     patience: 5
     factor: 0.5
   ```
   - R√©duction du taux d'apprentissage
   - Adaptation automatique

## üìä √âvaluation

### M√©triques

1. **Validation Crois√©e**
   ```yaml
   evaluation:
     validation_split: 0.2
     cross_validation:
       folds: 5
       shuffle: true
   ```

2. **Visualisation**
   ```yaml
   visualization:
     tensorboard:
       enabled: true
       log_dir: "logs/neural_network"
     plots:
       - "loss_curve"
       - "gradient_flow"
       - "feature_importance"
       - "confusion_matrix"
   ```

## üíª Exemples d'Utilisation

### 1. Entra√Ænement Simple
```python
from trading.models import NeuralNetwork
from trading.data import DataPreparator

# Pr√©paration des donn√©es
preparator = DataPreparator()
X_train, X_test, y_train, y_test = preparator.prepare_data(market_data)

# Cr√©ation et entra√Ænement du mod√®le
model = NeuralNetwork()
history = model.train(
    X_train=X_train,
    y_train=y_train,
    X_val=X_test,
    y_val=y_test
)
```

### 2. Personnalisation du Mod√®le
```python
# Configuration personnalis√©e
custom_config = {
    'architecture': {
        'layers': [
            {'size': 256, 'activation': 'relu', 'dropout': 0.4},
            {'size': 128, 'activation': 'relu', 'dropout': 0.3},
            {'size': 64, 'activation': 'relu', 'dropout': 0.2}
        ]
    },
    'training': {
        'batch_size': 512,
        'epochs': 200
    }
}

# Cr√©ation avec config personnalis√©e
model = NeuralNetwork(config=custom_config)
```

### 3. √âvaluation et Pr√©diction
```python
# √âvaluation
metrics = model.evaluate(X_test, y_test)
print(f"MSE: {metrics['mse']:.4f}")
print(f"Sharpe Ratio: {metrics['sharpe_ratio']:.2f}")

# Pr√©diction
predictions = model.predict(X_test)
```

## üîß Bonnes Pratiques

1. **Pr√©paration des Donn√©es**
   - Normalisation des features
   - Gestion des valeurs manquantes
   - Augmentation des donn√©es si n√©cessaire

2. **Architecture**
   - Commencer simple (2-3 couches)
   - Ajouter de la complexit√© progressivement
   - Monitorer l'overfitting

3. **Entra√Ænement**
   - Utiliser la validation crois√©e
   - Impl√©menter early stopping
   - Sauvegarder les meilleurs mod√®les

4. **Optimisation**
   - Grid search sur les hyperparam√®tres cl√©s
   - Validation sur diff√©rentes p√©riodes
   - Test sur des march√©s diff√©rents

## üöÄ Optimisation Avanc√©e

### 1. Recherche d'Hyperparam√®tres
```python
from trading.optimization import HyperParameterOptimizer

optimizer = HyperParameterOptimizer(model_class=NeuralNetwork)
best_params = optimizer.optimize(
    X_train, y_train,
    param_grid={
        'learning_rate': [0.001, 0.0001],
        'batch_size': [128, 256, 512],
        'n_layers': [2, 3, 4]
    }
)
```

### 2. Ensemble Learning
```python
from trading.models import EnsembleNeuralNetwork

ensemble = EnsembleNeuralNetwork(
    n_models=5,
    base_model=NeuralNetwork
)
ensemble.train(X_train, y_train)
```

## üìà Visualisation des R√©sultats

### 1. TensorBoard
```python
# Activation dans la config
visualization:
  tensorboard:
    enabled: true
    log_dir: "logs/neural_network"
```

### 2. M√©triques Personnalis√©es
```python
from trading.visualization import ModelVisualizer

visualizer = ModelVisualizer(model)
visualizer.plot_attention_weights()  # Pour les mod√®les avec attention
visualizer.plot_feature_importance()
visualizer.plot_prediction_distribution()
```

## ‚ö†Ô∏è Points d'Attention

1. **Gestion de la M√©moire**
   - Utiliser des g√©n√©rateurs pour les grands datasets
   - Nettoyer le GPU r√©guli√®rement
   - Optimiser la taille des batchs

2. **Stabilit√©**
   - Initialisation des poids
   - Gradient clipping
   - Learning rate scheduling

3. **Production**
   - Versioning des mod√®les
   - Monitoring des performances
   - Mise √† jour r√©guli√®re 